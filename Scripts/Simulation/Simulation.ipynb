{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97708f12",
   "metadata": {},
   "source": [
    "# 1. Description of Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f73d7f",
   "metadata": {},
   "source": [
    "For each sample size:\n",
    "\n",
    "* 1000 datasets:\n",
    "    * $T = \\left\\{100, 200, 400, 800, 1600, 3200\\right\\}$ independent markets per dataset\n",
    "    * $J = 25$ products per market\n",
    "        * Each product $j$ in each market $t$ has an observed characteristic vector $x_{jt} = (1, p_{jt}, d_{jt},  x_{jt}^1,  x_{jt}^2, \\xi_{jt}, z_{jt})$\n",
    "            * $p_{jt} \\sim Lognormal(0, 1)$: continuous \"price\" variable; lognormal distribution roughly mimics empirical distribution of prices\n",
    "            * $d_{jt}$: binary variable (group/nest 0 or 1)\n",
    "            * $x_{jt}^1, x_{jt}^2 \\sim U[0.1,5]$: continuous covariates\n",
    "            * $\\xi_{jt} \\sim N(0,1)$: unobserved product characteristic, correlated with the price\n",
    "            * $z_{jt} \\sim N(0,1)$: a simulated instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7204ce1",
   "metadata": {},
   "source": [
    "Treat $d_{jt}$ as the realization of a latent continuous variable $d_{jt}^*$, with $ d_{jt} = 1\\left\\{d_{jt}^* > 0\\right\\}$. Furthermore, $x_{jt}^i$, for $i=1,2$, is constructed by evenly positioning the instances of the latent variable $(x_{jt}^i)^*$ on the interval $[0.1,5].$ Specifically, assume:\n",
    "$$  \\begin{pmatrix} \\ln p \\\\ d^* \\\\ (x^1)^* \\\\  (x^2)^* \\\\ \\xi \\\\ z \\end{pmatrix}\n",
    "    \\sim\n",
    "    N\n",
    "    \\begin{pmatrix} \\begin{matrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{matrix},\n",
    "    \\begin{matrix}  1 & \\sigma & \\sigma & \\sigma & \\sigma & \\sigma \\\\\n",
    "                    \\sigma & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "                    \\sigma & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "                    \\sigma & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "                    \\sigma & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "                    \\sigma & 0 & 0 & 0 & 0 & 1 \\\\\\end{matrix} \\end{pmatrix} $$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc36c1",
   "metadata": {},
   "source": [
    "Preference parameters:\n",
    "\n",
    "* Utility weights on the price $\\alpha$ and on the remaining covariates and their interactions $\\beta = (\\beta_0, \\beta_d, \\beta_{x^1}, \\beta_{x^2}, \\beta_{dx^1}, \\beta_{dx^2})$\n",
    "* Nesting parameter $\\rho$, reflecting consumer heterogeneity in preferences for $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d1e4b",
   "metadata": {},
   "source": [
    "True DGP: Indirect utility of consumer $i$ from product $j$ of nest $g$ in market $t$:\n",
    "\n",
    "$$ v_{ijt} = \\beta_0 + \\alpha p_{jt} + \\beta_d d_{jt} + \\beta_{x^1} \\ln{x_{jt}^1} + \\beta_{x^2} \\ln{x_{jt}^2} + \\beta_{dx^1} d_{jt} \\ln{x_{jt}^1} + \\beta_{dx^2} d \\ln{x_{jt}^2} + \\sum_g{I\\left\\{d_{jt} = g\\right\\} \\zeta_{ig}} + (1 - \\rho) \\epsilon_{ijt} $$\n",
    "where\n",
    "\n",
    "* $I(d = g)$ is an indicator function that equals 1 when $d = g$\n",
    "* $\\zeta_{ig}$ is an individual-specific taste parameter common for consumer $i$ to all products in nest $g$\n",
    "* $\\epsilon_{ijt}$ is an individual-specific taste parameter for product $j$ that is i.i.d. Type-I Extreme Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f15f23",
   "metadata": {},
   "source": [
    "Estimated function derived from the nested logit model:\n",
    "\n",
    "$$ \\ln{s_{jt}} - \\ln{s_{0t}} = \\alpha p_{jt} + f(d_{jt}, x_{jt}^1, x_{jt}^2) + \\rho \\ln{s_{j/g,t}} + \\xi_{jt} $$\n",
    "where\n",
    "* $s_{jt}$ is the market share of product $j$ in market $t$\n",
    "* $s_{0t}$ is the outside option share in market $t$\n",
    "* $s_{j/g,t}$ is the within-nest share of product $j$ in nest $g$, in market $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df155b4",
   "metadata": {},
   "source": [
    "# 2. Setup: Defining the DGP and Estimation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c347443",
   "metadata": {},
   "source": [
    "Libraries and setting seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b344c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm            # for OLS\n",
    "from linearmodels import IV2SLS         # for 2SLS\n",
    "import pyblp                            # for differentiation instruments\n",
    "from econml.grf import CausalForest     # for causal forest\n",
    "import keras                            # for NNet\n",
    "from econml.iv.dml import DMLIV         # for DML with IV\n",
    "from econml.dml import LinearDML        # for DML\n",
    "import xgboost as xgb                   # for DML orthogonalization\n",
    "import time\n",
    "import os\n",
    "import functools\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (18, 12)\n",
    "\n",
    "seed = 1000\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d78c94",
   "metadata": {},
   "source": [
    "Timer function as a decorator for the estimations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63825e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(timed_function):\n",
    "    @functools.wraps(timed_function) # just to have func.__name__ return the function name rather than \"wrapper\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t1 = time.perf_counter()\n",
    "        result = timed_function(*args, **kwargs)\n",
    "        print(f'{timed_function.__name__} ({round(time.perf_counter() - t1, 2)})', end = ' ')\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60823f",
   "metadata": {},
   "source": [
    "## 2.1. Defining the data generating process and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7acb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = insert number\n",
    "class LogitSim:\n",
    "    \"\"\"Class for simulating the data\"\"\"\n",
    "    \n",
    "    \n",
    "    # Class Variables ----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    T = #SET QUANTITY TO RUN # number of markets\n",
    "    J = 25                   # number of products per market\n",
    "    beta = np.array([-3, -2, -1, 1, 1, 1, 1])  # utility parameters (including alpha)\n",
    "    rho = 0.3                # nesting parameter\n",
    "    z = 1      # number of instruments\n",
    "    c = 0.3    # correlation parameter in the DGP covariance matrix\n",
    "    \n",
    "    # Initializer --------------------------------------------------------------------------------------------------------\n",
    "    #    1. Product features\n",
    "    #    2. Resulting market shares and other NL estimation statistics\n",
    "    #    3. Resulting elasticities\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializer of the features data, including their market shares and\n",
    "        their own and cross price elasticities.\n",
    "        Returned attributes:\n",
    "            * sj, ln_sj: market share and its log\n",
    "            * ln_s0: outside option log market share\n",
    "            * ln_sjg: log within-nest market share\n",
    "            * y: NL outcome\n",
    "            * X: NL covariate matrix\n",
    "            * Z: excluded instruments matrix\n",
    "            * d_index: indicator matrix – 1 when d_i = d_j, 0 when d_i != d_j\n",
    "            * elasticities: true elasticity matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        T = self.T\n",
    "        J = self.J\n",
    "        beta = self.beta\n",
    "        rho = self.rho\n",
    "        z = self.z\n",
    "        c = self.c\n",
    "        \n",
    "        ###########################\n",
    "        #        Features\n",
    "        ###########################\n",
    "        p = 5 + z    # dimension of the DGP covariance matrix\n",
    "        covMat = np.random.multivariate_normal(mean = np.repeat(0, p),\n",
    "                                               cov = (np.identity(p) +\n",
    "                                                      np.hstack((0,\n",
    "                                                                 np.repeat(c, p-1),\n",
    "                                                                 np.repeat(0, (p-1)*p))).reshape(p,p) +\n",
    "                                                      np.hstack((0,\n",
    "                                                                 np.repeat(c, p-1),\n",
    "                                                                 np.repeat(0, (p-1)*p))).reshape(p,p).T),\n",
    "                                               size = T*J)\n",
    "        features = np.column_stack((\n",
    "            np.ones(T*J),                                                # constant\n",
    "            np.exp(covMat[:,0]),                                         # p\n",
    "            (covMat[:,1] > 0).astype(int),                               # d\n",
    "            np.log(((covMat[:,2].argsort().argsort()) / ((T*J - 1) / 4.9)) + 0.1),     # ln(x1) (evenly distributed on [0.1,5])\n",
    "            np.log(((covMat[:,3].argsort().argsort()) / ((T*J - 1) / 4.9)) + 0.1),     # ln(x2) (evenly distributed on [0.1,5])\n",
    "            (covMat[:,1] > 0).astype(int) * np.log(((covMat[:,2].argsort().argsort()) /\n",
    "                                                    ((T*J - 1) / 4.9)) + 0.1),    # d*ln(x1)\n",
    "            (covMat[:,1] > 0).astype(int) * np.log(((covMat[:,3].argsort().argsort()) /\n",
    "                                                    ((T*J - 1) / 4.9)) + 0.1),    # d*ln(x2)\n",
    "            covMat[:,4:p]                                                # xi, z\n",
    "        ))\n",
    "        \n",
    "        #############################################################\n",
    "        #      Market shares and other NL estimation statistics\n",
    "        #############################################################\n",
    "        # 1. Utility net of epsilon – dimensions (TxJ, N)\n",
    "        numerators = np.exp((features[:,0:7] @ beta.T + features[:,7]) / (1 - rho))\n",
    "\n",
    "        # 2. Computing denominators\n",
    "        D0 = np.repeat(np.sum((numerators * (features[:,2] == 0)).reshape(T,J), axis = 1), repeats = J, axis = 0)\n",
    "        D1 = np.repeat(np.sum((numerators * (features[:,2] == 1)).reshape(T,J), axis = 1), repeats = J, axis = 0)\n",
    "        Dg = np.where(features[:,2] == 1, D1, D0)\n",
    "        D  = D0**(1-rho) + D1**(1-rho) + 1\n",
    "        denominators = (Dg**rho) * D\n",
    "\n",
    "        # 3. Dividing to get the market shares\n",
    "        sj = numerators / denominators\n",
    "        ln_sj = np.log(sj)\n",
    "\n",
    "        # 4. Computing outside option share\n",
    "        s0 = np.repeat(1 - np.sum(sj.reshape(T, J), axis = 1), repeats = J)\n",
    "        ln_s0 = np.log(s0)\n",
    "\n",
    "        # 5. Computing within-nest shares (the nansum is needed for cases when there is only one group in a market)\n",
    "        sjg = np.nansum(\n",
    "            np.column_stack((\n",
    "                (sj * features[:,2]) / np.repeat(np.sum(sj.reshape(T, J) * features[:,2].reshape(T, J), axis = 1),\n",
    "                                                 repeats = J),\n",
    "                (sj * (1 - features[:,2])) / np.repeat(np.sum(sj.reshape(T, J) * (1 - features[:,2]).reshape(T, J), axis = 1),\n",
    "                                                       repeats = J)\n",
    "            )),\n",
    "            axis = 1\n",
    "        )\n",
    "        ln_sjg = np.log(sjg)\n",
    "        \n",
    "        # 6. NL outcome variable, X matrix, and Z (instruments) matrix\n",
    "        y = ln_sj - ln_s0\n",
    "        X = np.column_stack((features[:,0:3], np.exp(features[:,3:5])))   # constant, p, d, x1, x2\n",
    "        products_df = pd.DataFrame(\n",
    "            np.column_stack((sj, X[:,1:5], np.repeat(np.array([range(0,T)]), J), np.arange(0, T*J))),\n",
    "            columns = ['shares', 'prices', 'd', 'x1', 'x2', 'market_ids', 'firm_ids']\n",
    "        )\n",
    "        quadratic_instruments = pyblp.build_differentiation_instruments(\n",
    "            pyblp.Formulation('prices + d + x1 + x2'),\n",
    "            products_df,\n",
    "            version = 'quadratic'\n",
    "        )\n",
    "        Z = np.column_stack((features[:,8:], quadratic_instruments[:,7:10]))\n",
    "        \n",
    "        ###########################\n",
    "        #       Elasticities\n",
    "        ###########################\n",
    "        # Indicator matrix for whether the features are in the same nest\n",
    "        self.d_index = (np.repeat(features[:,2].reshape(T, J), repeats = J, axis = 0) + features[:,2].reshape(T*J, 1) != 1)\n",
    "\n",
    "        elasticities = (np.repeat((-beta[1] * sj * features[:,1]).reshape(T,J), repeats = J, axis = 0) *\n",
    "                        np.select(condlist   = [self.d_index,                # same nest\n",
    "                                                ~self.d_index],              # different nest\n",
    "                                  choicelist = [(rho*Dg**(rho-1)*D + 1 - rho).reshape(T*J, 1) / (1 - rho), 1]) +\n",
    "                        (beta[1] * features[:,1].reshape(T*J,1) / (1 - rho)) * np.tile(np.identity(J), reps = (T,1)))\n",
    "        \n",
    "        ############################\n",
    "        #    Returned Attributes\n",
    "        ############################\n",
    "        self.sj = sj\n",
    "        self.ln_sj = ln_sj\n",
    "        self.ln_s0 = ln_s0\n",
    "        self.ln_sjg = ln_sjg\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "        self.Z = Z\n",
    "        self.elasticities = elasticities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b3683",
   "metadata": {},
   "source": [
    "## 2.2. First Step Estimators: 2SLS specifications and DMLIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e9eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstStepEstimates:\n",
    "    \"\"\"Class for conducting the first step estimations (alpha and rho values;\n",
    "    in the linear cases also the remaining parameter estimates) and storing\n",
    "    their outputs. 3 models:\n",
    "        1. 2SLS with linear covariates\n",
    "        2. 2SLS with a log transformation of x1 and x2\n",
    "        3. DML-IV – parametrically estimating alpha and rho while\n",
    "           controlling nonparametrically for the covariates\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, alpha = None, rho = None, beta = None, elasticities = None, eta_summary = None):\n",
    "        self.alpha = [] if alpha is None else alpha\n",
    "        self.rho = [] if rho is None else rho\n",
    "        self.beta = [] if beta is None else beta\n",
    "        self.elasticities = elasticities\n",
    "        self.eta_summary = [] if eta_summary is None else eta_summary\n",
    "    \n",
    "    # Static Methods -----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # 1. Computing the simulated products' elasticities based on parameter estimates\n",
    "    # 2. Summarizing the elasticity data:\n",
    "    #        a. Computing the average own price elasticities, within-nest cross price elasticities, and the across-nest\n",
    "    #           cross price elasticities\n",
    "    #        b. Computing the root mean squared errors of each of these three elasticity groups\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_elasticities(SimData, alpha, rho):\n",
    "        D0 = np.repeat(np.sum((np.exp((SimData.ln_sj - SimData.ln_s0 - rho * SimData.ln_sjg) / (1 - rho)) * \n",
    "                               (SimData.X[:,2] == 0)).reshape(SimData.T, SimData.J), axis = 1),\n",
    "                       repeats = SimData.J, axis = 0)\n",
    "        D1 = np.repeat(np.sum((np.exp((SimData.ln_sj - SimData.ln_s0 - rho * SimData.ln_sjg) / (1 - rho)) * \n",
    "                               (SimData.X[:,2] == 1)).reshape(SimData.T, SimData.J), axis = 1),\n",
    "                       repeats = SimData.J, axis = 0)\n",
    "        Dg = np.where(SimData.X[:,2] == 1, D1, D0)\n",
    "        D  = 1 + np.where(D0 == 0, 0, D0**(1-rho)) + np.where(D1 == 0, 0, D1**(1-rho))\n",
    "\n",
    "        elasticities = \\\n",
    "            (np.repeat((-alpha * SimData.sj * SimData.X[:,1]).reshape(SimData.T, SimData.J),\n",
    "                       repeats = SimData.J, axis = 0) *\n",
    "                np.select(condlist   = [SimData.d_index,       # same nest\n",
    "                                        ~SimData.d_index],     # different nest\n",
    "                          choicelist = [(rho * Dg**(rho-1) * D + 1 - rho).reshape(SimData.T * SimData.J, 1) /(1 - rho), 1]) +\n",
    "                (alpha * SimData.X[:,1].reshape(SimData.T * SimData.J, 1) / (1 - rho)) *\n",
    "                np.tile(np.identity(SimData.J), reps = (SimData.T, 1)))\n",
    "        return elasticities\n",
    "    \n",
    "    @staticmethod\n",
    "    def eta_summary(SimData, eta_est):\n",
    "        avg_own         = np.mean(eta_est[np.tile(np.identity(SimData.J), reps = (SimData.T,1)) == 1])\n",
    "        avg_cross_same  = np.mean(eta_est[(np.tile(np.identity(SimData.J), reps = (SimData.T,1)) == 0) & (SimData.d_index == True)])\n",
    "        avg_cross_diff  = np.mean(eta_est[SimData.d_index == False])\n",
    "        rmse_own        = (np.mean((SimData.elasticities[np.tile(np.identity(SimData.J), reps = (SimData.T,1)) == 1] -\n",
    "                                    eta_est[np.tile(np.identity(SimData.J), reps = (SimData.T,1)) == 1])**2))**.5\n",
    "        rmse_cross_same = (np.mean((SimData.elasticities[(np.tile(np.identity(SimData.J), reps = (SimData.T,1)) == 0) & (SimData.d_index == True)] -\n",
    "                                    eta_est[(np.tile(np.identity(SimData.J), reps = (SimData.T,1)) == 0) & (SimData.d_index == True)])**2))**.5\n",
    "        rmse_cross_diff = (np.mean((SimData.elasticities[SimData.d_index == False] -\n",
    "                                    eta_est[SimData.d_index == False])**2))**.5\n",
    "        return np.array([avg_own, avg_cross_same, avg_cross_diff, rmse_own, rmse_cross_same, rmse_cross_diff])\n",
    "    \n",
    "    \n",
    "    # Methods ------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # First step estimates\n",
    "    #    a. Linear 2SLS\n",
    "    #    b. Linear 2SLS with log transfomration\n",
    "    #    c. DML-IV\n",
    "    \n",
    "    @timer\n",
    "    def LinIV(self, SimData):\n",
    "        theta = IV2SLS(dependent = SimData.y,\n",
    "                       exog = np.delete(SimData.X, 1, 1),\n",
    "                       endog = np.column_stack((SimData.X[:,1], SimData.ln_sjg)),\n",
    "                       instruments = SimData.Z).fit().params\n",
    "        self.alpha.append(theta[-2])  # endog vars always at the end\n",
    "        self.rho.append(theta[-1])\n",
    "        self.beta.append(theta[:-2])\n",
    "        self.elasticities = FirstStepEstimates.compute_elasticities(SimData, self.alpha[-1], self.rho[-1])\n",
    "        self.eta_summary.append(FirstStepEstimates.eta_summary(SimData, self.elasticities))\n",
    "    \n",
    "    @timer\n",
    "    def LinIVLog(self, SimData):\n",
    "        theta = IV2SLS(dependent = SimData.y,\n",
    "                       exog = np.column_stack((SimData.X[:,[0,2]], np.log(SimData.X[:,3:5]))),\n",
    "                       endog = np.column_stack((SimData.X[:,1], SimData.ln_sjg)),\n",
    "                       instruments = SimData.Z).fit().params\n",
    "        self.alpha.append(theta[-2])\n",
    "        self.rho.append(theta[-1])\n",
    "        self.beta.append(theta[:-2])\n",
    "        self.elasticities = FirstStepEstimates.compute_elasticities(SimData, self.alpha[-1], self.rho[-1])\n",
    "        self.eta_summary.append(FirstStepEstimates.eta_summary(SimData, self.elasticities))\n",
    "    \n",
    "    @timer\n",
    "    def DMLIV(self, SimData,\n",
    "              depth_yx, n_est_yx, lr_yx,\n",
    "              depth_tx, n_est_tx, lr_tx,\n",
    "              depth_txz, n_est_txz, lr_txz):\n",
    "        # Initializing the estimator\n",
    "        dmliv_est = DMLIV(model_y_xw  = xgb.XGBRegressor(max_depth = depth_yx, n_estimators = n_est_yx, learning_rate = lr_yx),\n",
    "                          model_t_xw  = xgb.XGBRegressor(max_depth = depth_tx, n_estimators = n_est_tx, learning_rate = lr_tx),\n",
    "                          model_t_xwz = xgb.XGBRegressor(max_depth = depth_txz, n_estimators = n_est_txz, learning_rate = lr_txz))\n",
    "        # Fitting and storing the alpha and rho values\n",
    "        dmliv_est.fit(Y = SimData.y,\n",
    "                      T = np.column_stack((SimData.X[:,1], SimData.ln_sjg)),\n",
    "                      Z = SimData.Z,\n",
    "                      X = None,\n",
    "                      W = SimData.X[:,2:5])\n",
    "        alpha_, rho_ = dmliv_est.const_marginal_effect().reshape(-1)\n",
    "        self.alpha.append(alpha_)\n",
    "        self.rho.append(rho_)\n",
    "        self.elasticities = FirstStepEstimates.compute_elasticities(SimData, self.alpha[-1], self.rho[-1])\n",
    "        self.eta_summary.append(FirstStepEstimates.eta_summary(SimData, self.elasticities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19903a79",
   "metadata": {},
   "source": [
    "## 2.3. Second Step Estimators: DML, GRF, and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fedbd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondStepEstimates:\n",
    "    \"\"\"Class for conducting the second step estimations and storing their\n",
    "    outputs.\n",
    "    This step estimates average treatment effects, conditional average\n",
    "    treatment effects, and marginal effects of the covariates, assuming the\n",
    "    alpha and rho values of the first step DML-IV estimation.\n",
    "        1. DML: returns an ATE estimate for a selected covariate in X\n",
    "        2. GRF: returns CATE estimates for a selected covariate in X (the\n",
    "           implementation here uses a regular causal forest; can be replaced\n",
    "           by a generalized random forest in the presence of an endogenous\n",
    "           covariate of interest)\n",
    "        3. MLP: returns marginal effect estimates for a selected covariate\n",
    "           in X\n",
    "            * Note: the MLP architecture is intentionally very wide relative\n",
    "              to the complexity of the problem, with the aim of mitigating\n",
    "              regularization bias caused by the early stopping rule. The\n",
    "              desire to avoid regularization bias also explains the absence\n",
    "              of other forms of regularization. This preference is due to\n",
    "              the nature of the problem at hand, which is interested in\n",
    "              quantifying a causal relationship rather than in prediction,\n",
    "              and therefore places more weight on bias reduction, at the\n",
    "              expense of the estimates' variance.\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, effect = None):\n",
    "        self.effect = [] if effect is None else effect\n",
    "    \n",
    "    # Methods ------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Second step estimates\n",
    "    #    a. DML (ATE)\n",
    "    #    b. GRF (CATE)\n",
    "    #    c. MLP (marginal effects)\n",
    "    \n",
    "    @timer\n",
    "    def DML(self, SimData, estimates, treatment,\n",
    "            depth_y, n_est_y, lr_y,\n",
    "            depth_t, n_est_t, lr_t):\n",
    "        # Initializing\n",
    "        dml_est = LinearDML(model_y = xgb.XGBRegressor(max_depth = depth_y, n_estimators = n_est_y, learning_rate = lr_y),\n",
    "                            model_t = xgb.XGBRegressor(max_depth = depth_t, n_estimators = n_est_t, learning_rate = lr_t))\n",
    "        # Fitting and returning the ATE of the covariate selected as the treatment\n",
    "        dml_est.fit(Y = SimData.y - SimData.ln_sjg * estimates.rho[-1] - SimData.X[:,1] * estimates.alpha[-1],\n",
    "                      T = SimData.X[:, treatment],\n",
    "                      X = None,\n",
    "                      W = np.delete(SimData.X[:,2:5], treatment - 2, 1))\n",
    "        ATE = dml_est.ate()\n",
    "        self.effect.append(ATE)\n",
    "    \n",
    "    @timer\n",
    "    def GRF(self, SimData, estimates, treatment, grid, depth):\n",
    "        # Specifying the model:\n",
    "        grf_NL = CausalForest(criterion='het',\n",
    "                           n_estimators=1000,\n",
    "                           min_samples_leaf=5,\n",
    "                           max_depth=depth,\n",
    "                           min_impurity_decrease = 0.0,\n",
    "                           max_samples=0.45,\n",
    "                           warm_start=False,\n",
    "                           inference=False,\n",
    "                           fit_intercept=True,\n",
    "                           subforest_size=4,\n",
    "                           honest=True,\n",
    "                           verbose=0,\n",
    "                           n_jobs=-1)\n",
    "        # Fitting:\n",
    "        grf_NL.fit(y = SimData.y - SimData.ln_sjg * estimates.rho[-1] - SimData.X[:,1] * estimates.alpha[-1],\n",
    "                   X = np.delete(SimData.X[:,2:5], treatment - 2, 1),\n",
    "                   T = SimData.X[:, treatment].reshape(-1,1))\n",
    "        # Grid of effects:\n",
    "        CATE = grf_NL.predict(grid)\n",
    "        self.effect.append(CATE)\n",
    "    \n",
    "    @timer\n",
    "    def MLP(self, SimData, estimates, grid):\n",
    "        # Loop to rerun the function if the weights diverge\n",
    "        marginal_effects = np.array([np.nan])\n",
    "        while np.sum(np.isnan(marginal_effects)) > 0:\n",
    "            # Specifying the model:\n",
    "            model = keras.Sequential()\n",
    "            model.add(keras.layers.Dense(128, activation = 'relu'))\n",
    "            model.add(keras.layers.Dense(64, activation = 'relu'))\n",
    "            model.add(keras.layers.Dense(1))\n",
    "            model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "            model.fit(SimData.X[:,2:5],\n",
    "                      SimData.y - SimData.ln_sjg * estimates.rho[-1] - SimData.X[:,1] * estimates.alpha[-1],\n",
    "                      epochs = 50,\n",
    "                      validation_split = 0.1,\n",
    "                      verbose = False,\n",
    "                      callbacks = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True))\n",
    "            preds = model.predict(gridNN).reshape(-1)\n",
    "            marginal_effects = np.diff(preds) * 10  # normalizing the gradient from a 0.1 change in x to 1\n",
    "        self.effect.append(marginal_effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3346c02",
   "metadata": {},
   "source": [
    "# 3. Monte Carlo Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6eefad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Lin_est    = FirstStepEstimates()\n",
    "LinLog_est = FirstStepEstimates()\n",
    "DMLIV_est  = FirstStepEstimates()\n",
    "DML_est_x  = SecondStepEstimates()\n",
    "DML_est_d  = SecondStepEstimates()\n",
    "GRF_est_x  = SecondStepEstimates()\n",
    "GRF_est_d  = SecondStepEstimates()\n",
    "MLP_est    = SecondStepEstimates()\n",
    "\n",
    "iterations = 2\n",
    "iteration_time = []\n",
    "# Specifying the grids on which to estimate the GRF and deep IV marginal effects\n",
    "gridRFx = np.array([(x,y) for x in range(2) for y in np.linspace(0.1, 5, 50)])\n",
    "gridRFd = np.array([(x,y) for x in np.linspace(0.1, 5, 50) for y in np.linspace(0.1, 5, 50)])\n",
    "gridNN  = np.array([(x,y,z) for x in range(2) for y in np.arange(1, 6, 2) for z in np.arange(0.1, 5.1, 0.1)])\n",
    "\n",
    "for i in range(iterations):\n",
    "    iter_start = time.perf_counter()\n",
    "    print(i + 1, end = ' ')\n",
    "    \n",
    "    # Simulating a new dataset\n",
    "    dataset = LogitSim()\n",
    "    \n",
    "    # First step estimators\n",
    "    Lin_est.LinIV(dataset)\n",
    "    LinLog_est.LinIVLog(dataset)\n",
    "    DMLIV_est.DMLIV(dataset,\n",
    "                    depth_yx, n_est_yx, lr_yx,     # SET VALUES TO RUN\n",
    "                    depth_tx, n_est_tx, lr_tx,     # SET VALUES TO RUN\n",
    "                    depth_txz, n_est_txz, lr_txz)  # SET VALUES TO RUN\n",
    "    \n",
    "    # Second step estimators\n",
    "    DML_est_x.DML(dataset, DMLIV_est, 3,  # treatment = x1\n",
    "                  depth_y, n_est_y, lr_y,          # SET VALUES TO RUN\n",
    "                  depth_t, n_est_t, lr_t)          # SET VALUES TO RUN\n",
    "    DML_est_d.DML(dataset, DMLIV_est, 2,  # treatment = d\n",
    "                  depth_y, n_est_y, lr_y,          # SET VALUES TO RUN\n",
    "                  depth_t, n_est_t, lr_t)          # SET VALUES TO RUN\n",
    "    GRF_est_x.GRF(dataset, DMLIV_est, 3,  # treatment = x1\n",
    "                  gridRFx, depth)                  # SET VALUES TO RUN\n",
    "    GRF_est_d.GRF(dataset, DMLIV_est, 2,  # treatment = d\n",
    "                  gridRFd, depth)                  # SET VALUES TO RUN\n",
    "    MLP_est.MLP(dataset, DMLIV_est, gridNN)\n",
    "    \n",
    "    iteration_time.append(time.perf_counter() - iter_start)\n",
    "    print('| Runtime:', np.round(iteration_time[i], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2be244",
   "metadata": {},
   "source": [
    "# 4. Saving Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0213cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = f'Estimates {LogitSim.T}x{LogitSim.J}'\n",
    "if not os.path.exists(export_dir):\n",
    "    os.makedirs(export_dir)\n",
    "\n",
    "# First step parameter estimates\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'alpha_NL': Lin_est.alpha,\n",
    "        'rho_NL': Lin_est.rho,\n",
    "        'alpha_NL_log': LinLog_est.alpha,\n",
    "        'rho_NL_log': LinLog_est.rho,\n",
    "        'alpha_NL_XGB': DMLIV_est.alpha,\n",
    "        'rho_NL_XGB': DMLIV_est.rho\n",
    "    }\n",
    ").to_csv(export_dir + '\\\\step1.csv', index = False)\n",
    "\n",
    "# Second step linear coefficients/ATE estimates\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'NL_theta_x': np.array(Lin_est.beta)[:,2],\n",
    "        'NL_theta_d': np.array(Lin_est.beta)[:,1],\n",
    "        'NL_theta_log_x': np.array(LinLog_est.beta)[:,2],\n",
    "        'NL_theta_log_d': np.array(LinLog_est.beta)[:,1],\n",
    "        'NL_XGB_theta_x': DML_est_x.effect,\n",
    "        'NL_XGB_theta_d': DML_est_d.effect\n",
    "    }\n",
    ").to_csv(export_dir + '\\\\step2_theta.csv', index = False)\n",
    "\n",
    "# Second step CATE estimates\n",
    "pd.DataFrame(np.array(GRF_est_x.effect).reshape(-1, gridRFx.shape[0])).to_csv(export_dir + '\\\\step2_GRF_x.csv', index = False)\n",
    "pd.DataFrame(np.array(GRF_est_d.effect).reshape(-1, gridRFd.shape[0])).to_csv(export_dir + '\\\\step2_GRF_d.csv', index = False)\n",
    "\n",
    "# Second step marginal effects estimates\n",
    "pd.DataFrame(np.array(MLP_est.effect).reshape(-1, gridNN.shape[0] - 1)).to_csv(export_dir + '\\\\step2_MLP.csv', index = False)\n",
    "\n",
    "# Elasticity estimates RMSEs\n",
    "pd.DataFrame(\n",
    "    np.array(Lin_est.eta_summary)[~np.isinf(Lin_est.eta_summary).any(axis = 1), 3:6],\n",
    "    columns = ['Own', 'Cross (same)', 'Cross (different)']\n",
    ").to_csv(export_dir + '\\\\lin_RMSE.csv', index = False)\n",
    "pd.DataFrame(\n",
    "    np.array(LinLog_est.eta_summary)[~np.isinf(LinLog_est.eta_summary).any(axis = 1), 3:6],\n",
    "    columns = ['Own', 'Cross (same)', 'Cross (different)']\n",
    ").to_csv(export_dir + '\\\\log_RMSE.csv', index = False)\n",
    "pd.DataFrame(\n",
    "    np.array(DMLIV_est.eta_summary)[~np.isinf(DMLIV_est.eta_summary).any(axis = 1), 3:6],\n",
    "    columns = ['Own', 'Cross (same)', 'Cross (different)']\n",
    ").to_csv(export_dir + '\\\\xgb_RMSE.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
